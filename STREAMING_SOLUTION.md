# æ€ç»´é“¾æµå¼è¾“å‡ºå®ç°æ–¹æ¡ˆ

## é—®é¢˜åˆ†æ

å½“å‰çš„ `/api/v1/analysis/stream` è™½ç„¶ä½¿ç”¨äº† SSEï¼Œä½†å®é™…æ˜¯**ä¼ªæµå¼**ï¼š
```python
# å½“å‰å®ç° (routes.py:224)
final_state = await graph.ainvoke(initial_state)  # âŒ ç­‰å¾…å…¨éƒ¨æ‰§è¡Œå®Œ

# ç„¶åæ‰å‘é€ (routes.py:226-236)
for step in thinking_steps:
    yield f"event: thinking_step\n..."
```

è¿™å¯¼è‡´ç”¨æˆ·éœ€è¦ç­‰å¾… 30-60 ç§’æ‰èƒ½çœ‹åˆ°ç¬¬ä¸€ä¸ªå­—ç¬¦ã€‚

---

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ LangGraph astream() - èŠ‚ç‚¹çº§æµå¼è¾“å‡ºï¼ˆæ¨èï¼‰

**ä¼˜ç‚¹ï¼š**
- âœ… æ¯ä¸ªèŠ‚ç‚¹å®Œæˆåç«‹å³æµå¼å‘é€
- âœ… é¦–å­—ç¬¦å“åº”å¿«ï¼ˆ1-3ç§’ï¼‰
- âœ… å®ç°ç®€å•ï¼Œä¸éœ€è¦ä¿®æ”¹èŠ‚ç‚¹ä»£ç 
- âœ… ç”¨æˆ·å¯ä»¥å®æ—¶çœ‹åˆ°å„ä¸ªèŠ‚ç‚¹çš„è¿›åº¦

**ä»£ç ä¿®æ”¹ï¼š**

#### 1. ä¿®æ”¹ `app/api/routes.py` çš„æµå¼ç«¯ç‚¹

```python
@router.get("/analysis/stream")
async def analyze_marketing_goal_stream(
    prompt: str,
    session_id: str = None
):
    """ä½¿ç”¨ LangGraph astream() å®ç°çœŸæ­£çš„æµå¼è¾“å‡º"""
    logger.info(f"Received streaming analysis request: {prompt}")

    async def event_generator() -> AsyncIterator[str]:
        try:
            # ä¼šè¯ç®¡ç†
            session_manager = get_session_manager()
            session = session_manager.get_or_create_session(session_id)
            memory_manager = session_manager.memory_manager
            llm_context = memory_manager.build_context_for_llm(session, prompt)
            is_modification = memory_manager.should_modify_intent(session, prompt)

            # åˆå§‹çŠ¶æ€
            initial_state = {
                "user_input": prompt,
                "thinking_steps": [],
                "conversation_context": llm_context,
                "is_modification": is_modification,
                "previous_intent": session.current_context.get("latest_intent") if session.turns else None
            }

            # ğŸ”¥ å…³é”®æ”¹åŠ¨ï¼šä½¿ç”¨ astream() ä»£æ›¿ ainvoke()
            graph = get_agent_graph()
            final_state = None

            async for output in graph.astream(initial_state):
                # output æ ¼å¼: {node_name: node_output}
                for node_name, node_output in output.items():
                    logger.info(f"Node {node_name} completed")

                    # å®æ—¶å‘é€å½“å‰èŠ‚ç‚¹çš„ thinking_steps
                    thinking_steps = node_output.get("thinking_steps", [])
                    if thinking_steps:
                        # æ‰¾åˆ°æœ€æ–°æ·»åŠ çš„ stepï¼ˆé€šå¸¸æ˜¯æœ€åä¸€ä¸ªï¼‰
                        latest_step = thinking_steps[-1]
                        step_event = {
                            "stepId": latest_step["id"],
                            "title": latest_step["title"],
                            "description": latest_step["description"],
                            "status": latest_step["status"]
                        }
                        yield f"event: thinking_step\n"
                        yield f"data: {json.dumps(step_event, ensure_ascii=False)}\n\n"

                    # ä¿å­˜æœ€ç»ˆçŠ¶æ€
                    final_state = node_output

            # å‘é€æœ€ç»ˆç»“æœ
            if final_state:
                audience_list = final_state.get("audience", [])
                metrics_data = final_state.get("metrics", {})
                response_text = final_state.get("final_response", "")
                intent = final_state.get("intent", {})

                # è½¬æ¢ä¸ºå“åº”æ ¼å¼
                audience = [
                    {
                        "id": u["id"],
                        "name": u["name"],
                        "tier": u["tier"],
                        "score": u["score"],
                        "recentStore": u["recentStore"],
                        "lastVisit": u["lastVisit"],
                        "reason": u["reason"],
                        "matchScore": u.get("matchScore")
                    }
                    for u in audience_list
                ]

                metrics = {
                    "audienceSize": metrics_data.get("audience_size", 0),
                    "conversionRate": metrics_data.get("conversion_rate", 0),
                    "estimatedRevenue": metrics_data.get("estimated_revenue", 0),
                    "roi": metrics_data.get("roi", 0),
                    "reachRate": metrics_data.get("reach_rate", 0),
                    "qualityScore": metrics_data.get("quality_score", 0)
                }

                # ä¿å­˜ä¼šè¯
                turn = ConversationTurn(
                    user_input=prompt,
                    intent=intent,
                    audience=audience_list,
                    metrics=metrics_data,
                    response=response_text
                )
                session.add_turn(turn)

                # å‘é€å®Œæˆäº‹ä»¶
                result = {
                    "session_id": session.session_id,
                    "audience": audience,
                    "metrics": metrics,
                    "response": response_text,
                    "timestamp": datetime.now().isoformat()
                }
                yield f"event: analysis_complete\n"
                yield f"data: {json.dumps(result, ensure_ascii=False)}\n\n"

                logger.info(f"Streaming completed for session {session.session_id}")

        except Exception as e:
            logger.error(f"Error during streaming: {e}", exc_info=True)
            error_event = {"error": str(e)}
            yield f"event: error\n"
            yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )
```

**æ•ˆæœï¼š**
- ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆintent_analysisï¼‰å®Œæˆå ~3ç§’ â†’ ç”¨æˆ·çœ‹åˆ°ç¬¬ä¸€ä¸ª thinking step
- ç¬¬äºŒä¸ªèŠ‚ç‚¹ï¼ˆfeature_extractionï¼‰å®Œæˆå ~6ç§’ â†’ ç”¨æˆ·çœ‹åˆ°ç¬¬äºŒä¸ª step
- ä¾æ­¤ç±»æ¨ï¼Œç”¨æˆ·èƒ½å®æ—¶çœ‹åˆ°è¿›åº¦

---

### æ–¹æ¡ˆ 2ï¼šLLM çº§åˆ«çš„æµå¼è¾“å‡º - å­—ç¬¦çº§æµå¼ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æƒ³çœ‹åˆ° LLM "æ‰“å­—"çš„æ•ˆæœï¼ˆåƒ ChatGPT é‚£æ ·ï¼‰ï¼Œéœ€è¦æ›´æ·±åº¦çš„æ”¹åŠ¨ã€‚

**ä¼˜ç‚¹ï¼š**
- âœ… æè‡´çš„é¦–å­—ç¬¦å“åº”ï¼ˆ< 1ç§’ï¼‰
- âœ… ç”¨æˆ·èƒ½çœ‹åˆ° AI "æ€è€ƒ"çš„è¿‡ç¨‹
- âœ… ä½“éªŒæœ€æ¥è¿‘ ChatGPT

**ç¼ºç‚¹ï¼š**
- âŒ éœ€è¦ä¿®æ”¹æ‰€æœ‰èŠ‚ç‚¹çš„ LLM è°ƒç”¨
- âŒ éœ€è¦å®æ—¶è§£æ JSON æµ
- âŒ å®ç°å¤æ‚åº¦è¾ƒé«˜

#### å®ç°æ­¥éª¤

##### 1. ä¿®æ”¹ LLMManager æ·»åŠ æµå¼æ–¹æ³•

```python
# app/models/llm.py

class LLMManager:
    """ç°æœ‰ä»£ç å·²ç»æœ‰ stream() æ–¹æ³•ï¼Œéœ€è¦æ·»åŠ é«˜çº§å°è£…"""

    async def analyze_intent_stream(self, prompt: str) -> AsyncIterator[dict]:
        """æµå¼åˆ†ææ„å›¾ï¼Œå®æ—¶è¿”å› JSON ç‰‡æ®µ"""
        full_response = ""
        async for chunk in self.chat_model.stream(prompt):
            full_response += chunk
            # å°è¯•è§£æéƒ¨åˆ† JSON
            try:
                # å¦‚æœèƒ½è§£æå°± yield
                partial_result = json.loads(full_response)
                yield {"type": "partial", "data": partial_result}
            except json.JSONDecodeError:
                # è¿˜æ²¡å®Œæ•´å°± yield æ–‡æœ¬
                yield {"type": "text", "data": chunk}

        # æœ€ç»ˆå®Œæ•´ç»“æœ
        try:
            final_result = json.loads(full_response)
            yield {"type": "complete", "data": final_result}
        except json.JSONDecodeError:
            yield {"type": "error", "data": "Failed to parse JSON"}
```

##### 2. ä¿®æ”¹èŠ‚ç‚¹æ”¯æŒæµå¼è¾“å‡º

```python
# app/agent/nodes.py

async def intent_analysis_node_stream(state: AgentState, emit_callback):
    """æ”¯æŒæµå¼è¾“å‡ºçš„æ„å›¾åˆ†æèŠ‚ç‚¹"""
    llm = get_llm_manager()
    user_input = state["user_input"]

    # å…ˆå‘é€ "æ­£åœ¨æ€è€ƒ" çš„çŠ¶æ€
    await emit_callback({
        "type": "thinking_step_update",
        "data": {
            "id": "1",
            "title": "ä¸šåŠ¡æ„å›¾ä¸çº¦æŸè§£æ",
            "description": "æ­£åœ¨åˆ†æè¥é”€ç›®æ ‡...",
            "status": "active"
        }
    })

    # æµå¼è°ƒç”¨ LLM
    full_intent = None
    async for chunk in llm.analyze_intent_stream(user_input):
        if chunk["type"] == "text":
            # å®æ—¶æ›´æ–°æè¿°
            await emit_callback({
                "type": "thinking_step_text",
                "data": {
                    "id": "1",
                    "text": chunk["data"]
                }
            })
        elif chunk["type"] == "complete":
            full_intent = chunk["data"]

    # å®Œæˆåå‘é€å®Œæ•´ step
    await emit_callback({
        "type": "thinking_step_complete",
        "data": {
            "id": "1",
            "title": "ä¸šåŠ¡æ„å›¾ä¸çº¦æŸè§£æ",
            "description": f"æ ¸å¿ƒKPIï¼š{full_intent['kpi']}...",
            "status": "completed"
        }
    })

    return {"intent": full_intent, "thinking_steps": [...]}
```

##### 3. ä½¿ç”¨ LangGraph astream_events()

```python
# app/api/routes.py

@router.get("/analysis/stream/llm")  # æ–°ç«¯ç‚¹
async def analyze_with_llm_streaming(prompt: str, session_id: str = None):
    """LLM çº§åˆ«çš„æµå¼è¾“å‡º"""

    async def event_generator() -> AsyncIterator[str]:
        try:
            # ... ä¼šè¯ç®¡ç† ...

            graph = get_agent_graph()

            # ğŸ”¥ ä½¿ç”¨ astream_events() è·å–æ‰€æœ‰äº‹ä»¶
            async for event in graph.astream_events(initial_state):
                event_type = event["event"]

                # æ•è·è‡ªå®šä¹‰äº‹ä»¶
                if event_type == "on_custom_event":
                    custom_type = event["data"]["type"]

                    if custom_type == "thinking_step_text":
                        # æµå¼å‘é€ LLM è¾“å‡ºçš„æ–‡æœ¬
                        yield f"event: llm_chunk\n"
                        yield f"data: {json.dumps(event['data'], ensure_ascii=False)}\n\n"

                    elif custom_type == "thinking_step_complete":
                        # å‘é€å®Œæ•´çš„ step
                        yield f"event: thinking_step\n"
                        yield f"data: {json.dumps(event['data']['data'], ensure_ascii=False)}\n\n"

            # ... å‘é€æœ€ç»ˆç»“æœ ...

        except Exception as e:
            # ... é”™è¯¯å¤„ç† ...

    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

---

## æ¨èå®æ–½è·¯å¾„

### é˜¶æ®µ 1ï¼šå¿«é€Ÿæ”¹è¿›ï¼ˆ1-2å°æ—¶ï¼‰
å®æ–½**æ–¹æ¡ˆ 1**ï¼šä½¿ç”¨ `astream()` å®ç°èŠ‚ç‚¹çº§æµå¼è¾“å‡º
- ä¿®æ”¹ `routes.py` çš„ `/analysis/stream` ç«¯ç‚¹
- æµ‹è¯•ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹å®Œæˆåèƒ½å®æ—¶å‘é€
- å‰ç«¯è°ƒæ•´ï¼šæ¥æ”¶å¹¶å±•ç¤ºå®æ—¶çš„ thinking steps

**é¢„æœŸæ•ˆæœï¼š**
- é¦–å­—ç¬¦å“åº”ä» 30-60ç§’ â†’ 3-5ç§’
- ç”¨æˆ·èƒ½çœ‹åˆ° 5 ä¸ªèŠ‚ç‚¹ä¾æ¬¡å®Œæˆçš„è¿›åº¦

### é˜¶æ®µ 2ï¼šæè‡´ä½“éªŒï¼ˆå¯é€‰ï¼Œ1-2å¤©ï¼‰
å®æ–½**æ–¹æ¡ˆ 2**ï¼šLLM çº§åˆ«çš„æµå¼è¾“å‡º
- ä¿®æ”¹æ‰€æœ‰èŠ‚ç‚¹æ”¯æŒ LLM æµå¼è°ƒç”¨
- ä½¿ç”¨ `astream_events()` æ•è·æ‰€æœ‰äº‹ä»¶
- å‰ç«¯æ”¯æŒå­—ç¬¦çº§çš„æ‰“å­—æ•ˆæœ

**é¢„æœŸæ•ˆæœï¼š**
- é¦–å­—ç¬¦å“åº” < 1ç§’
- ç”¨æˆ·èƒ½çœ‹åˆ° LLM "æ€è€ƒ"çš„å®æ—¶è¿‡ç¨‹
- ä½“éªŒç±»ä¼¼ ChatGPT

---

## å‰ç«¯è°ƒæ•´å»ºè®®

æ— è®ºä½¿ç”¨å“ªä¸ªæ–¹æ¡ˆï¼Œå‰ç«¯éƒ½éœ€è¦è°ƒæ•´äº‹ä»¶å¤„ç†ï¼š

```javascript
// æ–¹æ¡ˆ 1 çš„å‰ç«¯ä»£ç 
const eventSource = new EventSource(`/api/v1/analysis/stream?prompt=${prompt}&session_id=${sessionId}`);

eventSource.addEventListener('thinking_step', (event) => {
  const step = JSON.parse(event.data);
  // ç«‹å³åœ¨ UI ä¸Šæ˜¾ç¤ºæ–°çš„ thinking step
  addThinkingStep(step);
});

eventSource.addEventListener('analysis_complete', (event) => {
  const result = JSON.parse(event.data);
  // æ˜¾ç¤ºæœ€ç»ˆç»“æœ
  displayFinalResult(result);
  eventSource.close();
});

// æ–¹æ¡ˆ 2 çš„å‰ç«¯ä»£ç ï¼ˆé¢å¤–æ”¯æŒï¼‰
eventSource.addEventListener('llm_chunk', (event) => {
  const chunk = JSON.parse(event.data);
  // å®æ—¶è¿½åŠ  LLM è¾“å‡ºçš„æ–‡å­—
  appendTextToCurrentStep(chunk.id, chunk.text);
});
```

---

## æ€»ç»“

| æ–¹æ¡ˆ | é¦–å­—ç¬¦å“åº” | å®ç°éš¾åº¦ | ç”¨æˆ·ä½“éªŒ | æ¨èåº¦ |
|------|------------|----------|----------|--------|
| **å½“å‰** | 30-60ç§’ | - | â­â­ | - |
| **æ–¹æ¡ˆ 1** | 3-5ç§’ | ä½ | â­â­â­â­ | âœ… **æ¨è** |
| **æ–¹æ¡ˆ 2** | < 1ç§’ | é«˜ | â­â­â­â­â­ | å¯é€‰ |

**å»ºè®®ï¼š**
1. å…ˆå®æ–½æ–¹æ¡ˆ 1ï¼Œå¿«é€Ÿæå‡ä½“éªŒ
2. å¦‚æœç”¨æˆ·åé¦ˆå¥½ï¼Œå†è€ƒè™‘æ–¹æ¡ˆ 2 çš„æè‡´ä¼˜åŒ–
3. ä¿æŒå½“å‰çš„ `/api/v1/analysis` åŒæ­¥ç«¯ç‚¹ä¸å˜ï¼Œä½œä¸ºå¤‡é€‰
